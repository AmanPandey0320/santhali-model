{"cells":[{"cell_type":"markdown","metadata":{"id":"qDwN_-lI70K1"},"source":["# Install Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjwC2ox474Up"},"outputs":[],"source":["!pip install --upgrade torch\n","!pip install pytube\n","!pip install git+https://github.com/openai/whisper.git"]},{"cell_type":"markdown","metadata":{"id":"Zb6koY_A8lew"},"source":["# Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lytexUaQ8lG-"},"outputs":[],"source":["import torch\n","import whisper\n","import pytube\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import IPython.display as ipd"]},{"cell_type":"markdown","metadata":{"id":"RQiUFDbD8dqH"},"source":["# Conver Speech-To-Text"]},{"cell_type":"code","source":["model_type=\"small\"\n","df = pd.read_csv(\"./drive/MyDrive/Santhali-English NLP/data.csv\")\n","df.head()"],"metadata":{"id":"HmW8KVMLU1gZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_text(model_m,file_path):\n","  try:\n","    model_data = model_m.transcribe(file_path,language=\"en\", fp16 = False)\n","    print(model_data)\n","    text = model_data['text']\n","    return text\n","  except Exception as e:\n","    print(e)\n","    return False"],"metadata":{"id":"U-UNHKEQXHTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_sound(model_type):\n","  sound_folder_path = \"./drive/MyDrive/Santhali-English NLP/sounds/SNT_\"\n","  sound_extention=\".mp3\"\n","  model_m = whisper.load_model(model_type)\n","  new_df = []\n","  column_values = ['sn','en','pos']\n","  for i in range(len(df['english'])):\n","    print(\"reading sound \",i)\n","    sn,en,pos = to_text(model_m,sound_folder_path+str(i)+sound_extention),df['english'][i],df['pos'][i]\n","    new_df.append([sn,en,pos])\n","  sn_df,en_df,pos_df=[],[],[]\n","  for dd in new_df:\n","    sn_df.append(dd[0])\n","    en_df.append(dd[1])\n","    pos_df.append(dd[2])\n","  final_df = pd.DataFrame(list(zip(sn_df,en_df,pos_df)),columns=[\"sn\",\"en\",\"pos\"])\n","  final_csv_path = './drive/MyDrive/Santhali-English NLP/'+model_type+'.csv'\n","  with open(final_csv_path, 'w', encoding = 'utf-8-sig') as f:\n","    final_df.to_csv(f)\n","\n",""],"metadata":{"id":"nrise9veW43u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_sound(\"small\")"],"metadata":{"id":"Fjjzf2vtYT2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_sound(\"medium\")"],"metadata":{"id":"fH_tvktIdiuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["small_data_df = pd.read_csv(\"./drive/MyDrive/Santhali-English NLP/small.csv\")\n","medium_data_df = pd.read_csv(\"./drive/MyDrive/Santhali-English NLP/medium.csv\")"],"metadata":{"id":"KjZCO6Bhjr5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def merge_data_of_df(data1,data2)->int:\n","  if len(data1) > 40 and len(data2) > 40:\n","    return 0\n","  if len(data1) == 0 and len(data2) == 0:\n","    return 0\n","  elif len(data2) <= len(data1):\n","    return 2\n","  else:\n","    return 1"],"metadata":{"id":"V9jKBVFnmM3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["small_df_size,medium_df_size =  len(small_data_df),len(medium_data_df)\n","final_sn,final_en,final_pos = [],[],[]\n","for i in range(int(min(small_df_size,medium_df_size))):\n","  d = merge_data_of_df(str(small_data_df['sn'][i]),str(medium_data_df['sn'][i]))\n","  # print(d)\n","  if d == 0:\n","    continue\n","  elif d == 1:\n","    final_sn.append(str(small_data_df['sn'][i]))\n","    final_en.append(str(small_data_df['en'][i]))\n","    final_pos.append(str(small_data_df['pos'][i]))\n","  else:\n","    final_sn.append(str(medium_data_df['sn'][i]))\n","    final_en.append(str(medium_data_df['en'][i]))\n","    final_pos.append(str(medium_data_df['pos'][i]))\n","final_merge_df = pd.DataFrame(list(zip(final_sn,final_en,final_pos)),columns=[\"sn\",\"en\",\"pos\"])"],"metadata":{"id":"FAgiz_QtkEI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# final_merge_df.head()"],"metadata":{"id":"Reza-Rn31Qy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_csv_path = './drive/MyDrive/Santhali-English NLP/final_merged_data.csv'\n","with open(final_csv_path, 'w', encoding = 'utf-8-sig') as f:\n","  final_merge_df.to_csv(f)"],"metadata":{"id":"JeNPSixE2jRw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model\n"],"metadata":{"id":"7fnC6bZC3EXf"}},{"cell_type":"markdown","source":["## Data Cleaing"],"metadata":{"id":"tRx82gxh3vNz"}},{"cell_type":"code","source":["import string\n","import re\n","from pickle import dump\n","from unicodedata import normalize\n","from numpy import array"],"metadata":{"id":"qsd0jwDg5Goh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataSet = pd.read_csv(\"./drive/MyDrive/Santhali-English NLP/final_merged_data.csv\")"],"metadata":{"id":"J2NrPHui3InI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_pairs(df):\n","  all_sn,all_en = df['sn'],df['en']\n","  pairs = list()\n","  for i in range(len(all_sn)):\n","    pairs.append([all_en[i],all_sn[i]])\n","  return pairs"],"metadata":{"id":"_dFsdA6F38e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load data in form of sn - en pair\n","pairs_data = to_pairs(dataSet)\n","print(pairs_data[-5:],len(pairs_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lEcVSJZz4hES","executionInfo":{"status":"ok","timestamp":1676431635909,"user_tz":-330,"elapsed":413,"user":{"displayName":"Aman Kr Pandey","userId":"05287797838861171875"}},"outputId":"b64ad012-ccec-433b-d2cc-11b20a4c33f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['their-2', ' Thank you'], ['they-2', ' onaudience.'], ['they', ' love, Sonam.'], ['weak', ' So'], ['hair', ' Oh!']] 468\n"]}]},{"cell_type":"code","source":["# clean a list of lines\n","def to_clean_pairs(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t# normalize unicode characters\n","\t\t\tline = normalize('NFD', str(line)).encode('ascii', 'ignore')\n","\t\t\tline = line.decode('UTF-8')\n","\t\t\t# tokenize on white space\n","\t\t\tline = line.split()\n","\t\t\t# convert to lowercase\n","\t\t\tline = [word.lower() for word in line]\n","\t\t\t# remove punctuation from each token\n","\t\t\tline = [word.translate(table) for word in line]\n","\t\t\t# remove non-printable chars form each token\n","\t\t\tline = [re_print.sub('', w) for w in line]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn array(cleaned)"],"metadata":{"id":"WKDgECxS46AF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## getting cleaned data set\n","clean_pairs = to_clean_pairs(pairs_data)\n","print(clean_pairs[-5:],len(clean_pairs))"],"metadata":{"id":"1olNzsMR5dTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Split Dataset"],"metadata":{"id":"doGqUtZ_7r1N"}},{"cell_type":"code","source":["from pickle import load\n","from pickle import dump\n","from numpy.random import rand\n","from numpy.random import shuffle\n","\n","dataset = clean_pairs.copy()\n","total_data_size = len(dataset)\n","training_data_size = int(total_data_size * 0.75)\n","\n","shuffle(dataset)\n","\n","# split into train/test\n","train_data_set, test_data_set = dataset[:training_data_size], dataset[training_data_size:]"],"metadata":{"id":"PwFN3RW57ts5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train NLP"],"metadata":{"id":"a97EoBaa-0yy"}},{"cell_type":"code","source":["from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.callbacks import ModelCheckpoint"],"metadata":{"id":"bJ1k4jxC_JD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer"],"metadata":{"id":"AO5a0c_i-i4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)"],"metadata":{"id":"Xm_-dehfAF64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X"],"metadata":{"id":"E94A2vmpAKVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# one hot encode target sequence\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y"],"metadata":{"id":"jArTrSYXAN-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define model\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","\tmodel.add(LSTM(n_units))\n","\tmodel.add(RepeatVector(tar_timesteps))\n","\tmodel.add(LSTM(n_units, return_sequences=True))\n","\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","\treturn model"],"metadata":{"id":"ONnVy-7EARUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare sn tokenizer\n","sn_tokenizer = create_tokenizer(dataset[:, 0])\n","sn_vocab_size = len(sn_tokenizer.word_index) + 1\n","sn_length = max_length(dataset[:, 0])\n","print('SN Vocabulary Size: %d' % sn_vocab_size)\n","print('SN Max Length: %d' % (sn_length))\n","# prepare en tokenizer\n","en_tokenizer = create_tokenizer(dataset[:, 1])\n","en_vocab_size = len(en_tokenizer.word_index) + 1\n","en_length = max_length(dataset[:, 1])\n","print('EN Vocabulary Size: %d' % en_vocab_size)\n","print('EN Max Length: %d' % (en_length))"],"metadata":{"id":"LbW-KR3qAY_l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676431673703,"user_tz":-330,"elapsed":529,"user":{"displayName":"Aman Kr Pandey","userId":"05287797838861171875"}},"outputId":"62ae42dd-a1e8-4b34-9efb-6512bd91e11c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SN Vocabulary Size: 398\n","SN Max Length: 4\n","EN Vocabulary Size: 457\n","EN Max Length: 6\n"]}]},{"cell_type":"code","source":["# prepare training data\n","trainX = encode_sequences(en_tokenizer, en_length, train_data_set[:, 1])\n","trainY = encode_sequences(sn_tokenizer, sn_length, train_data_set[:, 0])\n","trainY = encode_output(trainY, sn_vocab_size)"],"metadata":{"id":"ed9WAqWiBCLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare validation data\n","testX = encode_sequences(en_tokenizer, en_length, test_data_set[:, 1])\n","testY = encode_sequences(sn_tokenizer, sn_length, test_data_set[:, 0])\n","testY = encode_output(testY, sn_vocab_size)"],"metadata":{"id":"lN8ZTuDUBQOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define model\n","model = define_model(en_vocab_size, sn_vocab_size, en_length, sn_length, 256)\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","# summarize defined model\n","print(model.summary())\n","plot_model(model, to_file='model.png', show_shapes=True)"],"metadata":{"id":"WyKczfkbBdA2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit model\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"],"metadata":{"id":"i_DrfajcCnrs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"UkKXWx7LC6Yh"}},{"cell_type":"code","source":["from pickle import load\n","from numpy import array\n","from numpy import argmax\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu"],"metadata":{"id":"zvEmprntC2e-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer"],"metadata":{"id":"gMmuFw7uDAme"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)"],"metadata":{"id":"v3poqrGrDkue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X"],"metadata":{"id":"0kukRT2DDl7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None"],"metadata":{"id":"pEIZIYQEDosB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate target given source sequence\n","def predict_sequence(model, tokenizer, source):\n","\tprediction = model.predict(source, verbose=0)[0]\n","\tintegers = [argmax(vector) for vector in prediction]\n","\ttarget = list()\n","\tfor i in integers:\n","\t\tword = word_for_id(i, tokenizer)\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\ttarget.append(word)\n","\treturn ' '.join(target)"],"metadata":{"id":"kEopU8poDvDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate the model\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","\tactual, predicted = list(), list()\n","\tfor i, source in enumerate(sources):\n","\t\t# translate encoded source text\n","\t\tsource = source.reshape((1, source.shape[0]))\n","\t\ttranslation = predict_sequence(model, sn_tokenizer, source)\n","\t\traw_target, raw_src = raw_dataset[i]\n","\t\tactual.append([raw_target.split()])\n","\t\tpredicted.append(translation.split())\n","\t# calculate BLEU score\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"],"metadata":{"id":"4djhCD4fDyh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare sn tokenizer\n","sn_tokenizer = create_tokenizer(dataset[:, 0])\n","sn_vocab_size = len(sn_tokenizer.word_index) + 1\n","sn_length = max_length(dataset[:, 0])\n","# prepare en tokenizer\n","en_tokenizer = create_tokenizer(dataset[:, 1])\n","en_vocab_size = len(en_tokenizer.word_index) + 1\n","en_length = max_length(dataset[:, 1])\n","# prepare data\n","trainX = encode_sequences(en_tokenizer, en_length, train_data_set[:, 1])\n","testX = encode_sequences(en_tokenizer, en_length, test_data_set[:, 1])"],"metadata":{"id":"PIa-CljXD4e_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load model\n","model = load_model('model.h5')\n","# test on some training sequences\n","print('train')\n","evaluate_model(model, sn_tokenizer, trainX, train_data_set)\n","# test on some test sequences\n","print('test')\n","evaluate_model(model, sn_tokenizer, testX, test_data_set)"],"metadata":{"id":"ol6X4knBEfRJ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"10CI3TSeT4BaOdLr_lLPHNFi4dWkjV3xM","authorship_tag":"ABX9TyOdAH02Pc3WzgPJcygDF/I8"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}